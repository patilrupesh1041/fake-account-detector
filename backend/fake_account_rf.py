# -*- coding: utf-8 -*-
"""Fake_Account_RF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFtR29YdNBw0VDsuF79myVfKwcYE2KlO
"""

# Cell 1: Install and import libs (run once)
#!pip install -q imbalanced-learn joblib

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    roc_auc_score,
    roc_curve,
    precision_recall_fscore_support
)
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from google.colab import files

# Cell 2: Load CSV - tries common uploaded paths then falls back to Colab upload
possible_paths = [
    "/mnt/data/975c6c46-c78e-4c5c-bff9-7b8fcce62669.csv",
    "/mnt/data/eeac6e60-c902-4208-b51c-ad04ae5a68e1.csv",
    "/content/user_fake_authentic_4class.csv",
    "/content/user_fake_authentic_2class.csv",
    "dataset.csv"
]

df = None
for p in possible_paths:
    if os.path.exists(p):
        print("Loading:", p)
        df = pd.read_csv(p)
        break

if df is None:
    print("No dataset found in common paths. Please upload a CSV file now (choose the file).")
    uploaded = files.upload()
    # take the first uploaded file
    first_filename = next(iter(uploaded.keys()))
    df = pd.read_csv(first_filename)

print("Dataset shape:", df.shape)
df.head()

# Cell 3: Quick EDA
print("Columns:", df.columns.tolist())
print("\nMissing values per column:\n", df.isnull().sum())
print("\nBasic stats:\n", df.describe().T)

# If there's a label column not obvious, try some common names:
possible_label_cols = [c for c in df.columns if c.lower() in ("label","target","is_fake","fake","bot","class","y")]
print("\nDetected possible label columns:", possible_label_cols)

# Cell 4: set label and features
# Replace 'label' below with your actual label column name if different.
# Common names: 'label', 'is_fake', 'target', 'bot'
label_col = None
for candidate in ["label","is_fake","isfake","fake","target","bot","class","y"]:
    if candidate in df.columns:
        label_col = candidate
        break

if label_col is None:
    # If no standard label found, print columns and stop
    raise ValueError("No label column auto-detected. Rename your label column to one of: "
                     "label, is_fake, fake, target, bot, class, y or set `label_col` manually in the notebook.")

print("Using label column:", label_col)

# Drop identifier columns if present
id_cols = [c for c in df.columns if c.lower() in ("userid","user_id","id","user")]
features = [c for c in df.columns if c not in id_cols + [label_col]]
print("Dropped id cols:", id_cols)
print("Using features:", features)

X = df[features].copy()
y = df[label_col].copy()

# Simple cleaning: fill na with median for numeric, mode for categorical
for col in X.columns:
    if X[col].dtype in [np.float64, np.float32, np.int64, np.int32]:
        X[col] = X[col].fillna(X[col].median())
    else:
        X[col] = X[col].fillna(X[col].mode().iloc[0])

# Ensure numeric dtypes
X = X.apply(pd.to_numeric, errors='coerce').fillna(0)

print("Final X shape:", X.shape, "y shape:", y.shape)
print("Label distribution:\n", y.value_counts())

# Cell 5: split and apply SMOTE to training set only
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = pd.Series(le.fit_transform(y))

print("Encoded classes:", list(le.classes_))
RANDOM_STATE = 42
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE
)

print("Before SMOTE - train labels:\n", y_train.value_counts())

sm = SMOTE(random_state=RANDOM_STATE)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print("After SMOTE - train labels:\n", y_train_res.value_counts())
print("Unique classes in y_train_res:", np.unique(y_train_res))
print("Unique classes in y_test:", np.unique(y_test))

# Cell 6: baseline RandomForest in a pipeline (scaling -> RF)
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("rf", RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1))
])

pipeline.fit(X_train_res, y_train_res)

y_pred = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("ROC AUC (OvR):", roc_auc_score(y_test, y_proba, multi_class='ovr'))
print("\nClassification report:\n", classification_report(y_test, y_pred))

# Cell 7: confusion matrix and ROC
from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# --- ROC Curve ---
y_proba = pipeline.predict_proba(X_test)

# If multi-class, we binarize labels
if len(le.classes_) > 2:
    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
    plt.figure(figsize=(6,4))
    for i, class_name in enumerate(le.classes_):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{class_name} (AUC = {roc_auc:.3f})")
    plt.plot([0,1], [0,1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve (One-vs-Rest)")
    plt.legend()
    plt.show()
else:
    # Binary classification
    fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
    plt.figure(figsize=(6,4))
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_proba[:,1]):.4f}")
    plt.plot([0,1], [0,1], '--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.show()

# Cell 8: Grid Search + Evaluation (Fixed for multi-class)

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

param_grid = {
    "rf__n_estimators": [100, 200],
    "rf__max_depth": [None, 10, 30],
    "rf__min_samples_split": [2, 5],
    "rf__min_samples_leaf": [1, 2]
}

# Grid Search (can be slow)
gs = GridSearchCV(pipeline, param_grid, cv=3, scoring="accuracy", n_jobs=-1, verbose=2)
gs.fit(X_train_res, y_train_res)

print("Best params:", gs.best_params_)
best_model = gs.best_estimator_

# Evaluate on test data
y_pred_gs = best_model.predict(X_test)
y_proba_gs = best_model.predict_proba(X_test)

# If multiclass, calculate ROC AUC with OvR strategy
if y_proba_gs.shape[1] > 2:
    roc_auc = roc_auc_score(y_test, y_proba_gs, multi_class='ovr')
else:
    roc_auc = roc_auc_score(y_test, y_proba_gs[:, 1])

print("Test Accuracy:", accuracy_score(y_test, y_pred_gs))
print("Test ROC AUC (OvR):", roc_auc)
print("\nClassification Report:\n", classification_report(y_test, y_pred_gs))

# Cell 9: Feature importance
# Extract RF (after scaler) - pipeline named step 'rf'
rf = (gs.best_estimator_ if 'gs' in globals() else pipeline).named_steps['rf']
importances = rf.feature_importances_
feat_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)
print(feat_importance.head(20))

plt.figure(figsize=(8,5))
feat_importance.head(15).plot(kind="barh")
plt.gca().invert_yaxis()
plt.title("Top 15 Feature Importances")
plt.show()

# Cell 10: save trained model to disk
model_to_save = gs.best_estimator_ if 'gs' in globals() else pipeline
save_path = "/content/random_forest_fake_account_detector.joblib"
joblib.dump(model_to_save, save_path)
print("Saved model to:", save_path)

# Provide download link through Colab
files.download(save_path)